{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PriMAT: Multi-animal tracking\n",
        "\n",
        "In this notebook, we want to demonstrate how to train a tracking model from a few hundred frames. We will train a model that is able to track lemurs and feeding boxes with labelled images which can be downloaded [here](https://owncloud.gwdg.de/index.php/s/Mq4m9k1B74cN6ys) (-> Training Images / LemurBoxSep22 )."
      ],
      "metadata": {
        "id": "A0Atdc4Fr6C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mY2JvaUBk8ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
        "!pip install yacs==0.1.8\n",
        "!pip install opencv-python\n",
        "!pip install progress==1.6\n",
        "!pip install scikit-learn==1.2.2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SrxawMA3lDGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj9s06hC5_Yq"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ecker-lab/PriMAT-tracking.git\n",
        "%cd PriMAT-tracking/\n",
        "!mkdir data\n",
        "!mkdir models\n",
        "!mkdir exp\n",
        "!mkdir videos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "### Extract data"
      ],
      "metadata": {
        "id": "AE-cGOn0x53f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to upload the folder with the data to the colab space on the left. I uploaded it as a tar file and extracted it into the folder \"data\". The only important thing is that after this step you have training material in the folder data."
      ],
      "metadata": {
        "id": "dRfHHdpsslRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WbTCdH-fx_WR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data\n",
        "!tar -xvf ../../LemurBoxSep22.tar >/dev/null!tar\n",
        "%cd .."
      ],
      "metadata": {
        "id": "ix7jA_T4CL62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run training script"
      ],
      "metadata": {
        "id": "2Z6s9wDSyA_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell you start the training. Before this you have to make sure a few things:\n",
        "- The data_cfg file has to point to the correct location of your data. Open it (it is in PriMAT-tracking/src/lib/cfg/lemur-box.json) and adapt the root (for me it is /content/PriMAT-tracking/data/ in colab).\n",
        "- Give your experiment a name (--exp_id) so that you can find the model afterwards in exp/mot/exp_id/."
      ],
      "metadata": {
        "id": "eZF9kulKs868"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd src\n",
        "python train.py mot --exp_id colab_test\\\n",
        "                    --load_model ''\\\n",
        "                    --num_epochs 10\\\n",
        "                    --lr_step 5\\\n",
        "                    --lr '1e-4'\\\n",
        "                    --data_cfg '../src/lib/cfg/lemur_box.json'\\\n",
        "                    --store_opt\\\n",
        "                    --arch hrnet_32\\\n",
        "                    --gpus 0\\\n",
        "                    --batch_size 2\\\n",
        "                    --seed 13\\\n",
        "                    --reid_cls_names lemur,box\\\n",
        "                    --val_intervals 10\\\n",
        "                    --save_all\n",
        "cd .."
      ],
      "metadata": {
        "id": "06vNX0r2EQFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your models will be saved in exp/mot/exp_id and end with .pth."
      ],
      "metadata": {
        "id": "76ahWFSUyEfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "### Apply to videos\n",
        "\n",
        "If you want to have video output, you will need to activate ffmpeg."
      ],
      "metadata": {
        "id": "tdb8Vxcyya4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y ffmpeg"
      ],
      "metadata": {
        "id": "UURjc1qhyRh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If you want the model you just trained, you can directly change the path to ../exp/mot/exp_id/model_last.pth (or any other model you want). Alternatively, we can use the pretrained lemur model from [here](https://owncloud.gwdg.de/index.php/s/Mq4m9k1B74cN6ys) (-> Models).\n",
        "- You can upload your own videos or a validation video (e.g. Eval8.mp4) into data/Videos. Videos can be downloaded [here](https://owncloud.gwdg.de/index.php/s/Mq4m9k1B74cN6ys) (-> ValidationVideos/lemur_videos_eval/Videos/).\n",
        "- If you set output_format to video, a video will be saved to output_root/output_name. If you set it to text, only the tracking output as a .txt file will be saved."
      ],
      "metadata": {
        "id": "gg8AQ_g30V7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install motmetrics==1.2.0\n",
        "!pip install lap==0.4.0\n",
        "!pip install cython_bbox==0.1.3"
      ],
      "metadata": {
        "id": "cNU0lI5x2HEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd src\n",
        "\n",
        "python demo.py mot  --load_tracking_model ../models/lemur_tracking.pth\\\n",
        "                    --conf_thres 0.02\\\n",
        "                    --det_thres 0.5\\\n",
        "                    --new_overlap_thres 0.8\\\n",
        "                    --sim_thres 0.8\\\n",
        "                    --input_video ../data/Videos/Eval8.mp4\\\n",
        "                    --output_root ../videos/test/\\\n",
        "                    --output_name test_video\\\n",
        "                    --store_opt\\\n",
        "                    --line_thickness 2\\\n",
        "                    --debug_info\\\n",
        "                    --arch hrnet_32\\\n",
        "                    --output_format video\\\n",
        "                    --reid_cls_names \"lemur,box\"\\\n",
        "                    --proportion_iou 0.2\\\n",
        "                    --double_kalman\n",
        "\n",
        "\n",
        "cd .."
      ],
      "metadata": {
        "id": "5T2o99sFyq7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}