{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brown-investigation",
   "metadata": {},
   "source": [
    "## Image Pairs dataset\n",
    "\n",
    "This notebook takes random image pairs from videos, and saves them as a new dataset. An image pair are two frames which are either directly consecutive or within a few frames from each other.\n",
    "The number of image pairs taken from a video depends on the estimated number of monkeys in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imported-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "if '/usr/users/vogg/.conda/envs/fairmot/lib/python3.8/site-packages' not in sys.path:\n",
    "    sys.path.insert(0, '/usr/users/vogg/.conda/envs/fairmot/lib/python3.8/site-packages')\n",
    "    sys.path.insert(0, '/usr/users/vogg/FairMOT/src')\n",
    "    sys.path.insert(0, '/usr/users/vogg/FairMOT/src/lib')\n",
    "\n",
    "from models.model import create_model, load_model\n",
    "from models.decode import mot_decode, _nms, _topk\n",
    "from utils.post_process import ctdet_post_process\n",
    "from utils.image import get_affine_transform\n",
    "from models.utils import _tranpose_and_gather_feat\n",
    "from datasets.jde import letterbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operating-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function estimates the number of monkeys for a given frame\n",
    "\n",
    "def estimate_monkeys(frame_num):\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "    ret, img0 = cap.read()\n",
    "\n",
    "\n",
    "    # Scale and fill grey borders\n",
    "    img_box, _, _, _ = letterbox(img0, height=608, width=1088)\n",
    "\n",
    "    # Normalize RGB\n",
    "    img = img_box[:, :, ::-1].transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "    img /= 255.0\n",
    "\n",
    "    im_blob = torch.from_numpy(img).cuda().unsqueeze(0)\n",
    "\n",
    "    width = img0.shape[1]\n",
    "    height = img0.shape[0]\n",
    "    inp_height = im_blob.shape[2]\n",
    "    inp_width = im_blob.shape[3]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(im_blob)[-1]\n",
    "        hm = output['hm'].sigmoid_()\n",
    "        wh = output['wh']\n",
    "        reg = output['reg']\n",
    "\n",
    "    dets, inds = mot_decode(hm, wh, reg=reg, ltrb=True, K=50)\n",
    "\n",
    "    # remove detections below the threshold\n",
    "\n",
    "    remain_inds = dets[:, :, 4] > 0.4\n",
    "\n",
    "    total = remain_inds.cpu().numpy().sum()\n",
    "    \n",
    "    return(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "biblical-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir /usr/users/agecker/datasets/macaque_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blank-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/usr/users/agecker/datasets/macaque_videos_vogg/\"\n",
    "\n",
    "output_path = \"/usr/users/agecker/datasets/macaque_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "architectural-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4_list = []\n",
    "for item in sorted(os.listdir(path)):\n",
    "    if item.endswith(\".mp4\"):\n",
    "        mp4_list.append(item)\n",
    "\n",
    "#Remove videos which are in the validation set    \n",
    "rm_list = ['VID_20210223_123630.mp4', 'VID_20210223_123817.mp4', 'VID_20210223_123854.mp4', \n",
    "        'VID_20210224_115455.mp4', 'VID_20210224_114038.mp4', 'VID_20210224_115729.mp4',\n",
    "          'VID_20210227_133251.mp4', 'VID_20210227_133440.mp4', 'VID_20210228_153846.mp4']\n",
    "\n",
    "mp4_list = list(set(mp4_list) - set(rm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "unlimited-narrative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fuzzy-firewall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ../models/mcqcp/model_150.pth, epoch 150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = create_model('dla_34', heads =  {'hm': 1, 'wh': 4, 'id': 128, 'reg': 2}, \n",
    "                     head_conv = 256)\n",
    "\n",
    "model = load_model(model, '../models/mcqcp/model_150.pth')\n",
    "model = model.to(torch.device('cuda'))\n",
    "model.eval()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "perceived-elite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for n, video in enumerate(mp4_list):\n",
    "\n",
    "    cap = cv2.VideoCapture(path + video)\n",
    "\n",
    "    #Total number of frames\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    #width  = cap.get(cv2.CAP_PROP_FRAME_WIDTH)   # float `width`\n",
    "    #height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float `height`\n",
    "    breaks = np.max([1, int(np.floor(frame_count / 300))])\n",
    "    steps = int(np.floor(frame_count / breaks))\n",
    "    \n",
    "    est_list = []\n",
    "\n",
    "    for i in range(breaks):\n",
    "        est_list.append(estimate_monkeys(i * steps))\n",
    "\n",
    "    #max number of monkeys detected\n",
    "    max_monkeys = np.max(est_list)\n",
    "\n",
    "    #we will pull max_monkeys frames out of the video\n",
    "    samples = np.max([1, max_monkeys])\n",
    "    steps = int(np.floor(frame_count / samples))\n",
    "    \n",
    "    \n",
    "    for i in range(samples):\n",
    "\n",
    "        #decide if the pair is 1,2,5,10 or 20 frames apart\n",
    "        rand_step = int(np.random.choice([1,2,5,10,20], size=1, p=[0.6,0.2,0.14,0.05,0.01]))\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * steps)\n",
    "        ret, img0 = cap.read()\n",
    "        cv2.imwrite(output_path + \"img_%s_0.jpg\" % str(count).zfill(5), img0)     \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * steps + rand_step)\n",
    "        ret, img1 = cap.read()\n",
    "        cv2.imwrite(output_path + \"img_%s_1.jpg\" % str(count).zfill(5), img1) \n",
    "        count = count + 1\n",
    "    \n",
    "    if n%20 == 0:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-bullet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
